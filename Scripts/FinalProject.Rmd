---
title: "Final Project"
author: "Caleb Miller"
date: "2025-04-29"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Getting Started

First things first, I will have to load in tidyverse, download my data, and also get a look at my working directory
```{r}
#Load tidyverse

library(tidyverse)

#Getting my working directory

getwd()

#Reading in my data, it should already be placed in DataRaw, under XLSX

unzip(zipfile = "Data/DataRaw/XLSX/WI_Tree_Phenology.zip", exdir = "Data/DataRaw/XLSX/")

```
Uh oh, its already not working. Why? Because these are xlsx files, not csv files. Now, I could just open each file and save them as csv's, but I figured this is a good opportunity early-on to introduce a loop where I can take in the files individually and convert them to csv.

Is it probably simpler to just save the files? Yeah, probably.

Do I have to do extra work to get the proper packages? Yeah.

But I am justifying this as a nice way to get loops in here and expand my education a bit (I also just want to see if I can do it.)

```{r}
#install.packages("readxl") # This is the package needed to read the excel documents

#If you don't have the readxl package, you can download it here
```

```{r}
#Now we can get started with creating the for loop

library(readxl)

#I found a trick where I can assign directories names instead of typing every time

XLFolder <- "Data/DataRaw/XLSX/WI_Tree_Phenology/"
CSVFolder <- "Data/DataRaw/CSV/"

#Here you make the vector of the names of the files

XLFiles <- list.files(XLFolder, pattern = ".xlsx")
XLFiles

#Finally, the loop that creates the CSVs from XLSXs

for(i in XLFiles) {
  readxl::read_excel(paste0(XLFolder, i)) %>% #allows it to read in the XLSX doc
  write.csv(., paste0(CSVFolder, gsub(".xlsx", ".csv", i))) #tells it to take the XLSX doc, change it to a CSV by changing .xlsx to .csv, then put it in the folder
}
```

Was that possibly more difficult than need be? Probably but I did it and I feel awesome.

Now, the next thing I like to do is get a good look at the docs. To do that, I'm goint to read them in, but only get the heads of each one

```{r}
#Reading in the files themselves

SpringCoords <- read_csv("../Data/DataRaw/CSV/spring_coordinate_data.csv")
head(SpringCoords)
PhenoCodes <- read_csv("../Data/DataRaw/CSV/WI_Spring_Phenophase_Data_Codes.csv")
head(PhenoCodes)
TreeCoords <- read_csv("../Data/DataRaw/CSV/WI_Spring_Tree_Coordinates.csv")
head(TreeCoords)
PhenoAF <- read_csv("../Data/DataRaw/CSV/WI_Tree_Phenology_A_F.csv")
head(PhenoAF)
PhenoGL <- read_csv("../Data/DataRaw/CSV/WI_Tree_Phenology_G_L.csv")
head(PhenoGL)
PhenoMR <- read_csv("../Data/DataRaw/CSV/WI_Tree_Phenology_M_R.csv")
head(PhenoMR)
PhenoSX <- read_csv("../Data/DataRaw/CSV/WI_Tree_Phenology_S_X.csv")
head(PhenoSX)

PhenoSX
```

Great! Now we can look at all the data and come to some conculsions! Chiefly, there were certainly some decisions that were made with this data set. First and foremost, SpringCoords and TreeCoords are the same! Don't need two identical data tables in any situation. Further, all the Pheno data is broken up into 4 different tables sorted by A-F, G-L, M-R, and S-X. And on top of that, we do not need the 5th column at all and the data is all funky, specifically the way its writen includes the species of the tree, which is recorded in the TreeCoords (redundant data).

Other than that, there's other clean up (-99 instead of NA, excel messed up the date data) but first we need to format the data in a way that doesn't make my head hurt.

I believe the most important place to start would be merging all the Phenos into one table, that way whatever I do to it, I do to all of them.

Which is easier said than done, because these tables are Messed Up.

```{r}
#First things first, I'm going to play around with PhenoAF a bit to see if i can format it the way I want

#I want to fix one thing right away: the really weird date columns and weird first column. But how do I do that?
#I don't need that first row, its just column names or data I have in other sheets
#Same with the Tree ID Column

PhenoAF <- PhenoAF[-1, -5]%>%  #Gets rid of unnecessary columns
  rename(Year = `...2`, #renames columns
         Date = `...3`,
         DOY = `...4`)

PhenoAF$DOY <- as.numeric(PhenoAF$DOY) #changes DOY to numbers instead of characters

PhenoAF <- PhenoAF %>% 
  mutate(Date = as.Date(DOY-1, origin = paste0(Year, "-01-01"))) #gets actual date instead of messed-up Excel dates

PhenoAF
```

Now I'm going to do this to all of the remaining Pheno tables, but I don't wanna write all that out, so I'm going to take the above code and condense it into a function that'll clean them for me

```{r}
CleanMyTables <- function(Table) { #modifying my original cleaning code into a function
  RenameTable <- Table[-1, -5] %>%
    rename(Year = `...2`, 
         Date = `...3`,
         DOY = `...4`)
  RenameTable$DOY <- as.numeric(RenameTable$DOY)
  CleanTable <- RenameTable %>%
    mutate(Date = as.Date(DOY-1, origin = paste0(Year, "-01-01")))
  return(CleanTable)
}

PhenoGL <- CleanMyTables(PhenoGL) #Using them to just clean the tables for me
PhenoMR <- CleanMyTables(PhenoMR)
PhenoSX <- CleanMyTables(PhenoSX)

PhenoSX
```

Now: I still want to mix all the data together into one big table, so to do that, I'm going to use a full_join and join_by(Date)

```{r}
PhenoAX <- PhenoAF %>% 
  full_join(PhenoGL, join_by(Date)) %>% 
  full_join(PhenoMR, join_by(Date)) %>% 
  full_join(PhenoSX, join_by(Date))
PhenoAX
```

